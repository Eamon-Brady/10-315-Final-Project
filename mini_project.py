# -*- coding: utf-8 -*-
"""Mini Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SdlnHf3PoaJzH-BpGEBT-wPzgdrQ3Fkn

# Introduction

The problem we addressed was people often not knowing how to properly dispose of their trash. Depending on if it is glass, cardboard, or other types, there are various ways to dispose of it. Our project classifies images of trash, which in turn would allow people to know how to properly dispose of it.
  We used two different techniques to address this issue. First, we implemented a convolutional neural network to train on our dataset and then classify the images. Unlike a normal fully connected neural network, the convolutional network uses a filter with a fixed stride to examine various parts of the image, to account for images not having uniform scaling of the piece of trash they display. The other technique we used was a fully connected neural network to process the images and classify them. This uses multiple layers of neurons and backpropagation to train the model to properly process images and classify them.
"""

import shutil
import os
import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import re
import matplotlib.pyplot as plt
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import numpy as np  # linear algebra
import torchvision.transforms as transforms
import torchvision
import torch


"""As far as our dataset, we utilized a dataset including hundreds of images of six classes of trash including glass, plastic, paper, cardboard, trash, and metal. The dataset has hundreds of images of size 512 x 384 along with their proper class labels, including a training set and a separate testing set for our model to use.

The inputs to our problem are these images, which begin as 512 x 384, but are pre-processed to 224 x 224. The outputs are then the predicted class labels, which includes 6 separate probabilities for each class. So, our networks take in these images, perform the necessary processing steps, and then output the likelihood of them belonging to the various classes.

Our Two Techniques and Experimental Question

---



---

The two techniques that we utilized were a convolutional neural network and a fully connected neural network. The primary difference between these is the use of the filter with a stride on the convolutional neural network. This allows it to better account for differences in scale and rather than identify identical images, it can identify trends in the images and therefore better classify ones that are not identical to those we have trained on.

Our experimental question involves the effect of regularization on the quality of our models predictions. By adjusting the weight of an L2 regularization using the weight_decay function, we will assess how various values improve/worsen the quality of our model's predictions.
"""


# PyTorch and TorchVision packages


# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory


# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


# classes = ["cardboard", "glass", "metal", "paper", "plastic", "trash"]

# for className in classes:
#     directory = "./archive/Garbage classification/Garbage classification/" + className
#     dir = os.listdir(directory)
#     for i, img in enumerate(dir):
#         if i < len(dir) * 0.7:
#             shutil.copy(directory + "/" + img, "data/test/" + className)
#         else:
#             shutil.copy(directory + "/" + img, "data/train/" + className)

DATASET_ROOT = "./data"
SPLITS = ["train", "test"]
CLASSNAME_LABELS = ["cardboard", "glass", "metal", "paper", "plastic", "trash"]
SIZE = 224

"""# Pre-Trained Models

We have four models in total: a pretrained CNN (from MobileNet), a CNN with initially 0 weights, a fully connected neural network (or MLP), and a pretrained CNN with regularization (also from MobileNet). These models were all trained with 20 epochs and saved to our Github. Here, we load the models from Github and plot the training loss, training accuracy, validation loss and validation accuracy.

Note: Run all cells before running this code block.
"""

# def savedModel(pretrained=True):
#   model = GarbageMobileNet(pretrained)
#   if pretrained:
#     model.load_state_dict(torch.load("/content/10315-Final-Project/cnnWithPretrained.pth"))
#   else:
#     model.load_state_dict(torch.load("/content/10315-Final-Project/cnnWithoutPretrained.pth"))

#   trainDL, testDL, lossfn, opt = createTrainingUtils(model)

#   trainLoss, trainAcc, validLoss, validAcc = evaluate(model, trainDL, testDL, lossfn)
#   return (trainLoss, trainAcc, validLoss, validAcc)

# def savedModelMLP():
#   mobilenet = GarbageMLP()
#   mobilenet.load_state_dict(torch.load("/content/10315-Final-Project/mlp.pth"))

#   trainDL, testDL, lossfn, opt = createTrainingUtilsMLP(mobilenet)

#   trainLoss, trainAcc, validLoss, validAcc = evaluate(mobilenet, trainDL, testDL, lossfn)
#   return (trainLoss, trainAcc, validLoss, validAcc)

# def savedModelReg(pretrained=True):
#   mobilenet = mobilenet = GarbageMobileNet(pretrained)
#   mobilenet.load_state_dict(torch.load("/content/10315-Final-Project/cnnWithRegularization.pth"))

#   trainDL, testDL, lossfn, opt = createTrainingUtilsReg(mobilenet)

#   trainLoss, trainAcc, validLoss, validAcc = evaluate(mobilenet, trainDL, testDL, lossfn)
#   return (trainLoss, trainAcc, validLoss, validAcc)

# #Base code from GeeksForGeeks
# def drawBarGraph():
#   barWidth = 0.18
#   fig = plt.subplots(figsize =(12, 8))

#   # set height of bar
#   trained_CNN = list(savedModel(pretrained=True))
#   untrained_CNN = list(savedModel(pretrained=False))
#   reg_CNN = list(savedModelReg())
#   MLP = list(savedModelMLP())

#   # Set position of bar on X axis
#   br1 = np.arange(len(trained_CNN))
#   br2 = [x + barWidth for x in br1]
#   br3 = [x + barWidth for x in br2]
#   br4 = [x + barWidth for x in br3]

#   # Make the plot
#   plt.bar(br1, trained_CNN, color ='r', width = barWidth,
#           edgecolor ='grey', label ='Trained CNN')
#   plt.bar(br2, untrained_CNN, color ='g', width = barWidth,
#           edgecolor ='grey', label ='Untrained CNN')
#   plt.bar(br3, reg_CNN, color ='b', width = barWidth,
#           edgecolor ='grey', label ='Reg CNN')
#   plt.bar(br4, MLP, color = 'y', width= barWidth,
#           edgecolor = 'grey', label='MLP')

#   # Adding Xticks
#   plt.xlabel('', fontweight ='bold', fontsize = 15)
#   plt.ylabel('Percentage', fontweight ='bold', fontsize = 15)
#   plt.xticks([r + barWidth for r in range(len(trained_CNN))],
#           ['Traing Loss', 'Train Acc', 'Validation Loss', 'Validation Acc'])

#   plt.legend()
#   plt.show()

# drawBarGraph()

"""# Two Techniques

The two techniques that we utilized were a convolutional neural network and a fully connected neural network. The primary difference between these is the use of the filter with a stride on the convolutional neural network. This allows it to better account for differences in scale and rather than identify identical images, it can identify trends in the images and therefore better classify ones that are not identical to those we have trained on.

Our convolutional neural network uses techniques we learned in HW 7 to effectively predct the image classes. The convolutional neural network uses a kernel to analyze various sections of the image whereas the fully connected neural network just passes the images through a series of activation layers with parameters that are adjusted throughout training.
"""

# Utilizing similar structure to 10-315 Homework 7


class GarbageDataset(Dataset):

    def __init__(self, split, size=SIZE):
        ''' 
            - split: either "train" or "test"
            - size: size to reshape the image to (size, size)
        '''
        self.split = split
        self.size = size

        self.splitPath = os.path.join(DATASET_ROOT, split)
        self.imagePaths = [os.path.join(self.splitPath, cls, img) for cls in CLASSNAME_LABELS for img in os.listdir(
            os.path.join(self.splitPath, cls))]

    def __len__(self):
        '''Returns the length of the object

        Returns:
            - length: number of images in the dataset
        '''
        nsamples = len(self.imagePaths)
        return nsamples

    def __getitem__(self, idx):
        '''Returns two PyTorch tensors that are the image data and the target label for the image of that
        specific index within the image paths that are stored

        Arguments:
            - idx: the index of the sample, must be in [0, self.__len__())
        Returns:
            - img: PyTorch Tensor of dtype torch.float32 and of shape (3, self.size, self.size)
            - label: PyTorch Tensor of dtype torch.float32 and of shape (1) and contains either 0.0 or 1.0 
        '''
        imgPath = self.imagePaths[idx]

        def getClass(imgPath):
            index = 0
            for c in CLASSNAME_LABELS:
                if c in imgPath:
                    return index
                index += 1

        img = Image.open(imgPath)

        allTransforms = transforms.Compose([
            transforms.Resize((self.size, self.size)),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])

        img = allTransforms(img)
        img = torch.FloatTensor(img)

        y = getClass(imgPath)

        y_onehot = torch.zeros(6)
        y_onehot[y] = 1

        label = torch.FloatTensor(y_onehot)

        return img, label

# Utilizing similar structure to 10-315 Homework 7


def createTrainingUtils(model):
    '''Constructs the utilities that will be used to train the input model

    Arguments:
        - model: a nn.Module object that will be trained and used to define the optimizer
    Returns:
        - trainDL: an nn.DataLoader object initialized with a training HotdogDataset, batch size of 16, and shuffling enabled
        - testDL: an nn.DataLoader object initialized with a testing HotdogDataset, batch size of 16, and shuffling enabled
        - lossfn: a binary-cross entropy loss object used for calculating the loss between predicted and target values
        - opt: an Adam optimizer initialized with model parameters and a learning rate of 0.001, all other values are default
    '''
    opt = torch.optim.Adam(model.parameters(), lr=0.001)
    lossfn = torch.nn.BCELoss()
    trainDL = DataLoader(GarbageDataset("train"), batch_size=16, shuffle=True)
    testDL = DataLoader(GarbageDataset("test"), batch_size=16, shuffle=True)
    return trainDL, testDL, lossfn, opt

# Utilizing similar structure to 10-315 Homework 7


def createTrainingUtilsReg(model):
    '''Constructs the utilities that will be used to train the input model

    Arguments:
        - model: a nn.Module object that will be trained and used to define the optimizer
    Returns:
        - trainDL: an nn.DataLoader object initialized with a training HotdogDataset, batch size of 16, and shuffling enabled
        - testDL: an nn.DataLoader object initialized with a testing HotdogDataset, batch size of 16, and shuffling enabled
        - lossfn: a binary-cross entropy loss object used for calculating the loss between predicted and target values
        - opt: an Adam optimizer initialized with model parameters and a learning rate of 0.001, all other values are default
    '''
    opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)
    lossfn = torch.nn.BCELoss()
    trainDL = DataLoader(GarbageDataset("train"), batch_size=16, shuffle=True)
    testDL = DataLoader(GarbageDataset("test"), batch_size=16, shuffle=True)
    return trainDL, testDL, lossfn, opt

# Utilizing similar structure to 10-315 Homework 7


def createTrainingUtilsMLP(model):
    '''Constructs the utilities that will be used to train the input model

    Arguments:
        - model: a nn.Module object that will be trained and used to define the optimizer
    Returns:
        - trainDL: an nn.DataLoader object initialized with a training GarbageDataset, batch size of 16, and shuffling enabled
        - testDL: an nn.DataLoader object initialized with a testing GarbageDataset, batch size of 16, and shuffling enabled
        - lossfn: a binary-cross entropy loss object used for calculating the loss between predicted and target values
        - opt: an Adam optimizer initialized with model parameters and a learning rate of 0.001, all other values are default
    '''
    opt = torch.optim.Adam(model.parameters(), lr=0.001)
    lossfn = torch.nn.BCELoss()
    trainDL = DataLoader(GarbageDataset("train"), batch_size=16, shuffle=True)
    testDL = DataLoader(GarbageDataset("test"), batch_size=16, shuffle=True)
    return trainDL, testDL, lossfn, opt


# Utilizing similar structure to 10-315 Homework 7
model = torchvision.models.mobilenet_v2()
trainDL, testDL, lossfn, opt = createTrainingUtils(model)
print(type(trainDL), type(testDL), type(lossfn), type(opt), sep="\n")

# Utilizing similar structure to 10-315 Homework 7


class GarbageMobileNet(nn.Module):
    def __init__(self, pretrained=False):
        '''Initializer for the MobileNet model to predict on the GarbageDataset

        Arguments:
            - pretrained: boolean indicating whether to use pre-trained weights on ImageNet (weights="MobileNet_V2_Weights.IMAGENET1K_V1")
                          or random weights but still using the model architecture (weights=None)
        '''
        super().__init__()

        self.mobilenet = torchvision.models.mobilenet_v2(
            pretrained, progress=False)
        self.mobilenet.classifier[1] = nn.Linear(
            in_features=1280, out_features=6, bias=True)
        self.finalActivation = nn.Softmax(dim=1)

    def forward(self, x):
        '''Computes the model's predictions given data x

        Arguments:
            - x: a (batch_size, 3, SIZE, SIZE) floating point tensor (note: SIZE defined at top of notebook)
        Returns:
            - yhat: a (batch_size, 1) floating point tensor containing predictions on whether the each image is not hotdog or hotdog
        '''
        return self.finalActivation(self.mobilenet(x))
        # return self.mobilenet(x)

# Utilizing similar structure to 10-315 Homework 7


class GarbageMLP(nn.Module):
    def __init__(self, dropout_rate=0.5, weight_decay=0.01):
        super().__init__()
        self.fc1 = nn.Linear(in_features=3*SIZE*SIZE, out_features=512)
        self.bn1 = nn.BatchNorm1d(num_features=512)
        self.drop1 = nn.Dropout(p=dropout_rate)
        self.fc2 = nn.Linear(in_features=512, out_features=256)
        self.bn2 = nn.BatchNorm1d(num_features=256)
        self.drop2 = nn.Dropout(p=dropout_rate)
        self.fc3 = nn.Linear(in_features=256, out_features=128)
        self.bn3 = nn.BatchNorm1d(num_features=128)
        self.drop3 = nn.Dropout(p=dropout_rate)
        self.fc4 = nn.Linear(in_features=128, out_features=64)
        self.bn4 = nn.BatchNorm1d(num_features=64)
        self.drop4 = nn.Dropout(p=dropout_rate)
        self.fc5 = nn.Linear(in_features=64, out_features=6)
        self.softmax = nn.Softmax(dim=1)
        self.weight_decay = weight_decay

    def forward(self, x):
        x = x.view(-1, 3*SIZE*SIZE)
        x = F.relu(self.bn1(self.fc1(x)))
        x = self.drop1(x)
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.drop2(x)
        x = F.relu(self.bn3(self.fc3(x)))
        x = self.drop3(x)
        x = F.relu(self.bn4(self.fc4(x)))
        x = self.drop4(x)
        x = self.fc5(x)
        yhat = self.softmax(x)
        return yhat

    def weight_decay_loss(self):
        # calculate L2 regularization loss on all trainable parameters
        l2_loss = 0
        for param in self.parameters():
            if param.requires_grad and param.dim() > 1:
                l2_loss += torch.norm(param, p=2)
        return self.weight_decay * l2_loss

# Utilizing similar structure to 10-315 Homework 7


def evalDL(model, dl, lossfn):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    lossSum = 0
    nCorrect = 0
    with torch.no_grad():
        for x, y in dl:
            x = x.to(device)
            y = y.to(device)
            yhat = model(x)
            lossSum += lossfn(yhat, y) * x.shape[0]
            _, predictions = torch.max(yhat, 1)
            nCorrect += torch.sum(predictions == torch.max(y, 1)[1])
        numSamples = len(dl.dataset)
        avgLoss = lossSum / numSamples
        acc = nCorrect / numSamples
    return avgLoss.item(), acc.item()


def evaluate(model, trainDL, validDL, lossfn):
    trainLoss, trainAcc = evalDL(model, trainDL, lossfn)
    validLoss, validAcc = evalDL(model, validDL, lossfn)
    return trainLoss, trainAcc, validLoss, validAcc

# Utilizing similar structure to 10-315 Homework 7


def train(model, trainDL, validDL, lossfn, opt, epochs=10):
    '''Trains model and returns training and validation statistics

    Arguments:
        - model: nn.Module object whose parameters will be updated
        - trainDL: the nn.DataLoader object for the training dataset
        - validDL: the nn.DataLoader for the validation dataset
        - lossfn: a torch.nn loss function object used to evaluate how correct the model's predictions are
        - opt: a torch.optim.Optimizer object used for updating the model's parameters
        - epochs: the number of epochs to train for
    Returns:
        - model: the model after performing training and updating it's parameters
        - trainLosses: list s.t. trainLosses[i] = average loss on training dataset before epoch i
        - trainAccs: list s.t. trainAccs[i] = prediction accuracy on training dataset after epoch i
        - validLosses: list s.t. validLosses[i] = average loss on validation dataset after epoch i
        - validAccs: list s.t. validAccs[i] = prediction accuracy on validation dataset after epoch i
    '''

    trainLosses = []
    trainAccs = []
    validLosses = []
    validAccs = []
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    trainLoss, trainAcc, validLoss, validAcc = evaluate(
        model, trainDL, validDL, lossfn)
    trainLosses.append(trainLoss)
    trainAccs.append(trainAcc)
    validLosses.append(validLoss)
    validAccs.append(validAcc)
    print(
        f"Initial Model: train-loss={trainLoss:<4.2f} train-acc={trainAcc:<4.2f} valid-loss={validLoss:<4.2f} valid-acc={validAcc:<4.2f}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    for e in range(epochs):
        for X, y in trainDL:

            X = X.to(device)
            y = y.to(device)

            y_pred = model.forward(X)
            loss = lossfn(y_pred, y)

            opt.zero_grad()

            loss.backward()

            opt.step()

        trainLoss, trainAcc, validLoss, validAcc = evaluate(
            model, trainDL, validDL, lossfn)
        trainLosses.append(trainLoss)
        trainAccs.append(trainAcc)
        validLosses.append(validLoss)
        validAccs.append(validAcc)
        print(f"Epoch {e:>4d}: train-loss={trainLoss:<4.2f} train-acc={trainAcc:<4.2f} valid-loss={validLoss:<4.2f} valid-acc={validAcc:<4.2f}")

    return model, trainLosses, trainAccs, validLosses, validAccs

# Utilizing similar structure to 10-315 Homework 7


def trainMLP(model, trainDL, validDL, lossfn, opt, epochs=10):
    '''Trains model and returns training and validation statistics

    Arguments:
        - model: nn.Module object whose parameters will be updated
        - trainDL: the nn.DataLoader object for the training dataset
        - validDL: the nn.DataLoader for the validation dataset
        - lossfn: a torch.nn loss function object used to evaluate how correct the model's predictions are
        - opt: a torch.optim.Optimizer object used for updating the model's parameters
        - epochs: the number of epochs to train for
    Returns:
        - model: the model after performing training and updating it's parameters
        - trainLosses: list s.t. trainLosses[i] = average loss on training dataset before epoch i
        - trainAccs: list s.t. trainAccs[i] = prediction accuracy on training dataset after epoch i
        - validLosses: list s.t. validLosses[i] = average loss on validation dataset after epoch i
        - validAccs: list s.t. validAccs[i] = prediction accuracy on validation dataset after epoch i
    '''

    trainLosses = []
    trainAccs = []
    validLosses = []
    validAccs = []
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    trainLoss, trainAcc, validLoss, validAcc = evaluate(
        model, trainDL, validDL, lossfn)
    trainLosses.append(trainLoss)
    trainAccs.append(trainAcc)
    validLosses.append(validLoss)
    validAccs.append(validAcc)
    print(
        f"Initial Model: train-loss={trainLoss:<4.2f} train-acc={trainAcc:<4.2f} valid-loss={validLoss:<4.2f} valid-acc={validAcc:<4.2f}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    for e in range(epochs):
        for X, y in trainDL:
            X = X.to(device)
            y = y.to(device)

            y_pred = model.forward(X)
            loss = lossfn(y_pred, y)

            opt.zero_grad()

            loss.backward()

            opt.step()

        trainLoss, trainAcc, validLoss, validAcc = evaluate(
            model, trainDL, validDL, lossfn)
        trainLosses.append(trainLoss)
        trainAccs.append(trainAcc)
        validLosses.append(validLoss)
        validAccs.append(validAcc)
        print(f"Epoch {e:>4d}: train-loss={trainLoss:<4.2f} train-acc={trainAcc:<4.2f} valid-loss={validLoss:<4.2f} valid-acc={validAcc:<4.2f}")

    return model, trainLosses, trainAccs, validLosses, validAccs

# Utilizing similar structure to 10-315 Homework 7


def plotStatistics(pretrained, epochs, trainLosses, trainAccs, validLosses, validAccs):

    fig, axes = plt.subplots(1, 2)
    xdata = np.arange(epochs+1)

    axis = axes[0]
    axis.plot(xdata, trainLosses, label="Train")
    axis.plot(xdata, validLosses, label="Valid")
    axis.set_xlabel("# Epochs")
    axis.set_ylabel("Average Loss")
    axis.set_title(f"Loss w/ Pretrained: {pretrained}")
    axis.grid()
    axis.legend()

    axis = axes[1]
    axis.plot(xdata, trainAccs, label="Train")
    axis.plot(xdata, validAccs, label="Valid")
    axis.set_xlabel("# Epochs")
    axis.set_ylabel("Accuracy")
    axis.set_title(f"Accuracy w/ Pretrained: {pretrained}")
    axis.grid()
    axis.legend()

# Utilizing similar structure to 10-315 Homework 7


def plotStatisticsMLP(epochs, trainLosses, trainAccs, validLosses, validAccs):

    print(trainLosses)
    print(trainAccs)
    print(validLosses)
    print(validAccs)

    fig, axes = plt.subplots(1, 2)
    xdata = np.arange(epochs+1)

    axis = axes[0]
    axis.plot(xdata, trainLosses, label="Train")
    axis.plot(xdata, validLosses, label="Valid")
    axis.set_xlabel("# Epochs")
    axis.set_ylabel("Average Loss")
    axis.set_title("Loss}")
    axis.grid()
    axis.legend()

    axis = axes[1]
    axis.plot(xdata, trainAccs, label="Train")
    axis.plot(xdata, validAccs, label="Valid")
    axis.set_xlabel("# Epochs")
    axis.set_ylabel("Accuracy")
    axis.set_title("Accuracy")
    axis.grid()
    axis.legend()

# Utilizing similar structure to 10-315 Homework 7


def trainAndPlot(pretrained=False):
    '''Train the model and display the training and validation statistics

    1. initialize the GarbageMobileNet model (according to whether we are using the pretrained weights)
    2. initialize the training utilities such as the DataLoaders, loss function, and optimizer
    3. train the model for 10 epochs
    4. plot the statistics collected over training

    Arguments:
        - pretrained: whether MobileNet model will use pre-trained weights or not
    '''
    mobilenet = GarbageMobileNet(pretrained)
    trainDL, testDL, lossfn, opt = createTrainingUtils(mobilenet)
    model, trainLosses, trainAccs, validLosses, validAccs = train(
        mobilenet, trainDL, testDL, lossfn, opt, epochs=20)
    if pretrained:
        torch.save(model.state_dict(),
                   "./10315-Final-Project/cnnWithPretrained.pth")
    else:
        torch.save(model.state_dict(),
                   "./10315-Final-Project/cnnWithoutPretrained.pth")
    plotStatistics(pretrained, 20, trainLosses,
                   trainAccs, validLosses, validAccs)

# Utilizing similar structure to 10-315 Homework 7


def trainAndPlotReg(pretrained=True):
    '''Train the model and display the training and validation statistics

    1. initialize the GarbageMobileNet model (according to whether we are using the pretrained weights)
    2. initialize the training utilities such as the DataLoaders, loss function, and optimizer
    3. train the model for 10 epochs
    4. plot the statistics collected over training

    Arguments:
        - pretrained: whether MobileNet model will use pre-trained weights or not
    '''
    mobilenet = GarbageMobileNet(pretrained)
    trainDL, testDL, lossfn, opt = createTrainingUtilsReg(mobilenet)
    model, trainLosses, trainAccs, validLosses, validAccs = train(
        mobilenet, trainDL, testDL, lossfn, opt, epochs=20)
    torch.save(model.state_dict(),
               "./10315-Final-Project/cnnWithRegularization.pth")
    plotStatistics(pretrained, 20, trainLosses,
                   trainAccs, validLosses, validAccs)

# Utilizing similar structure to 10-315 Homework 7


def trainAndPlotMLP():
    '''Train the model and display the training and validation statistics

    1. initialize the GarbageMobileNet model (according to whether we are using the pretrained weights)
    2. initialize the training utilities such as the DataLoaders, loss function, and optimizer
    3. train the model for 10 epochs
    4. plot the statistics collected over training

    Arguments:
        - pretrained: whether MobileNet model will use pre-trained weights or not
    '''
    mobilenet = GarbageMLP()
    trainDL, testDL, lossfn, opt = createTrainingUtilsMLP(mobilenet)
    model, trainLosses, trainAccs, validLosses, validAccs = trainMLP(
        mobilenet, trainDL, testDL, lossfn, opt, 20)
    torch.save(model.state_dict(), "./10315-Final-Project/mlp.pth")
    plotStatisticsMLP(20, trainLosses, trainAccs, validLosses, validAccs)


"""# Two Techniques Results

Convolutional Neural Network

---




As can be seen above, the convolutional neural network works better than the fully connected neural network because it better accounts for the important features in the images that we are looking for, which is the pieces of trash themselves. Furthermore, the pretrained=true model for the convolutional neural network works even better, with a very low error rate, since it takes advantage of parameters found in previous training models.
"""

# trainAndPlot(pretrained=False)

# trainAndPlot(pretrained=True)

# trainAndPlotMLP()

# trainAndPlotReg()

"""# Experiment

# Citations

“Saving and Loading Models¶.” Saving and Loading Models - PyTorch Tutorials 2.0.0+cu117 Documentation, https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference.

“PYTORCH Documentation¶.” PyTorch Documentation - PyTorch 2.0 Documentation, https://pytorch.org/docs/stable/index.html.

10-315 Homework 7, 
https://www.cs.cmu.edu/~10315/assignments/hw10_blank.pdf
https://drive.google.com/file/d/1OqwUYFXzMDqs2wrDLKAc-1kjnPu_0Tn7/view
"""
